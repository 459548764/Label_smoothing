\section{Introduction}
At a very high-level, the only information provided by the label $y$ of an input
$x$ is that of all the other possible labels $y' \in [\![k]\!]$, $y$ is the most
befitting.

\subsection{Notations}
Labels are
denoted $y, y_i \in [\![k]\!]$, and the corresponding one-hot encodings are
denoted $\delta_{y},\delta_{y_i} \in \Delta_k$. For a class $j \in [\![K]\!]$,
$q(j)$ is the $j$ component of a vector $q \in \mathbb R^k$. For a neural net
$\theta$, $z(j|x;\theta)$ denotes the the $j$ component of the logit for input
$x$. $p(j|x)=\exp(z|x;\theta)/\sum_{k}\exp(z|x;\theta)$ is the softmax
distribution on the classes $j\in[\![K]\!]$.
